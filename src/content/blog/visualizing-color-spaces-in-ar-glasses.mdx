---
title: "Visualizing Color Spaces in Augmented Reality with Spectacles"
description: "Building interactive 3D color space visualizations for Snap Spectacles AR glasses to help painters understand pigment mixing and color gamuts."
pubDatetime: 2025-12-22T09:00:00Z
tags:
  [
    "augmented reality",
    "ar",
    "spectacles",
    "lens studio",
    "ui",
    "color space",
    "vfx editor",
  ]
ogImage: /assets/visualizing-color-spaces-in-ar-glasses/full_demo_og.gif
icon: /assets/visualizing-color-spaces-in-ar-glasses/ghost_color_space.png
draft: true
---

import WorkflowDiagram from "@components/WorkflowDiagram";
import WorkflowDiagramSimple from "@components/WorkflowDiagramSimple";
import ColorMixingChallenge from "@components/ColorMixingChallenge";
import CollapsibleVideo from "@components/CollapsibleVideo";
import HoverReveal from "@components/HoverReveal";
import CollapsibleCode from "@components/CollapsibleCode";
import Chapter from "@components/Chapter";
import ArticleTimeline from "@components/ArticleTimeline";

<ArticleTimeline
  client:load
  sections={[
    { id: "chapter-1", label: "The Challenge of Color Mixing", type: "chapter" },
    { id: "color-mixing-challenge", label: "Color Mixing Challenge", type: "challenge", parent: "chapter-1" },
    { id: "chapter-2", label: "Building Color Spaces", type: "chapter" },
    { id: "vfx-particles", label: "VFX Particles", type: "subsection" },
    { id: "4-procedural-meshes", label: "Procedural Meshes", type: "subsection" },
  ]}
/>

<div style="text-align: center;">
	<a
		href="https://github.com/a-sumo/specs-samples/"
		style="display: inline-flex; align-items: center; gap: 10px;"
	>
		<img
			src="/assets/visualizing-color-spaces-in-ar-glasses/ghost_color_space.png"
			width="80"
			alt="Color Space Sample Project"
		/>
		<span>a-sumo/specs-samples</span>
	</a>
</div>

<div style={{ display: "flex", justifyContent: "center", margin: "32px 0" }}>
<video
	controls
	autoPlay
	loop
	muted
	playsInline
	style={{ maxWidth: "300px", borderRadius: "12px", boxShadow: "0 4px 20px rgba(0,0,0,0.15)" }}
>
	<source src="/assets/visualizing-color-spaces-in-ar-glasses/full_demo.mp4" type="video/mp4" />
</video>
</div>

<Chapter client:load id="chapter-1" number={1} title="The Challenge of Color Mixing">

One of the first steps a painter takes is that of assembling together the colors that will be laid on the canvas.
This involves selecting a set of pigments, laying them out on a palette and mixing them. 
During this process, each decision moves the color of the paint across three dimensions: hue, saturation(chromaicity) and value (lightness).
In theory, these dimensions are independent, or orthogonal. This means you can change the value along one dimension while keeping others constant.
Most digital color pickers are built on this assumption: you can pick a hue on one axis and adjust saturation and lightness on another.

<div style={{ textAlign: "center", margin: "16px 0" }}>
<img
	src="/assets/visualizing-color-spaces-in-ar-glasses/figma_hsl_color_picker.png"
	alt="Figma HSL color picker"
	style={{ width: "200px", borderRadius: "8px" }}
/>
<p style={{ fontSize: "14px", color: "rgb(100, 100, 100)", marginTop: "8px" }}>Figma's HSL color picker: hue on one slider, saturation and lightness on a 2D plane.</p>
</div>

Physical pigments, however are not as simple. For example, in order to de-saturate the green color of a tree as it vanishes into the horizon, you'll generally want to add a colder color to it, such as blue.
But by adding blue, you also shift the hue towards... blue! This means that in practice, the dimensions aren't actually independent.
It turns out that mixing color pigments involves a kind of multidimensional dance, which experienced painters perform seamlessly.
Performing this process adequately has an impact on the confidence of your brushstrokes and the overall fluidity of your artwork.
In fact, the more problems the painter can solve ahead of laying their brush on the canvas, the more enjoyable the process for them and the more free-flowing your painting will _feel_ to the viewer.

Now, some shortcuts can be taken, namely using very few or lots of pigments.
If you have too few pigments, you have little expressiveness and it may be unclear whether a certain color in your reference is even achievable.
On the other hand, the more pigments you have, the more degrees of freedom, and hence more complexity. 
This complexity grows combinatorially: not only do you have more possible mixtures to consider, but many different mixtures can yield the same color!
This makes it much harder to reason about which path to take. For a beginner, it's recipe for disaster.

Wanna try? Here: I'll give you a color and you need to find out which mix achieves it!

<div style={{ textAlign: "center" }}>
<CollapsibleVideo client:load src="/assets/visualizing-color-spaces-in-ar-glasses/videos/bonne_chance_franco-succession.mp4" preText='"' trigger="Bonne chance!" postText='"' description="Succession, Season 3 Episode 5" />
</div>

<div id="color-mixing-challenge" style={{ scrollMarginTop: "80px" }}>
<ColorMixingChallenge client:load targetColor="#8B4513" />
</div>

Now, it should be no surprise to you that mastering the pigment mixing process takes years of practice.
You just can't develop an intuition for how specific color pigments will interact with one another across hue saturation and value just by watching video tutorials and reading books. You must paint, again and again, ideally with some guidance, but knowing that the process will be tedious.

But how tedious does it really _need_ to be? Is frustration necessary for learning, or can skillful effort be sufficient?

Let us formulate our problem statement:

<div id="problem-statement" style="background: rgb(255, 248, 222); border: 2px solid rgb(40, 39, 40); padding: 20px 24px; borderRadius: 8px; margin: 24px 0;">
<ol style="margin: 0; paddingLeft: 20px; color: rgb(40, 39, 40); lineHeight: 1.8;">
<li id="q-color-range"><strong>What does the full range of perceivable colors look like?</strong></li>
<li id="q-gamut"><strong>Which colors can I achieve with the pigments on my palette?</strong></li>
<li id="q-projection"><strong>How close can I get to a target color given my available pigments?</strong></li>
</ol>
</div>

</Chapter>

<Chapter client:load id="chapter-2" number={2} title="Building Color Spaces">

The challenges I described above are well-suited to what Bret Victor calls [Seeing Spaces](https://worrydream.com/SeeingSpaces/): environments that make invisible relationships visible by giving you immediate, spatial representations of abstract data. Instead of holding mental models in your head and guessing at outcomes, you see the entire possibility space laid out before you.

For color mixing, this means visualizing all three color dimensions simultaneously as you work. The tool for this is a color space: a 3D coordinate system where every possible color occupies a unique position.
There are a wide variety of color spaces, but the ones that matter for painting are those that are perceptually accurate.
That's because transitions between colors end up informing mixing decisions.

But seeing spaces only _really_ work when they're seamlessly integrated into the user's environment. They must also be highly responsive and conceptually powerful.
That's a lot to ask from a smartphone, tablet or desktop. None of these devices can be integrated into the painting flow without disrupting it.

Fortunately, we are in the age of AR glasses and I got a pair of [Spectacles](https://www.spectacles.com/).

<div style={{ display: "flex", justifyContent: "center", margin: "24px 0" }}>
<img
	src="/assets/visualizing-color-spaces-in-ar-glasses/specs.png"
	alt="Snap Spectacles AR glasses"
	style={{ maxWidth: "400px", borderRadius: "8px" }}
/>
</div>

With the hardware sorted, I needed to decide what exactly to visualize. I had three goals in mind:
- See where a color on my palette sits in the color space, so when I change its mixture, I can track the result.
- See which colors are reachable by mixing my available pigments, via what is called a [color gamut](https://en.wikipedia.org/wiki/Gamut).
- See how a target color maps onto my gamut: what's the closest I can get to a reference color with my pigments?

The sRGB space, which we'll improperly abbreviate as RGB, is the obvious starting point, but because of its perceptual inaccuracy, it's a poor fit for this problem.

[CIELAB](https://en.wikipedia.org/wiki/CIELAB_color_space), for instance, is better suited: it's designed so that equal distances in the space correspond to equal perceived color differences. 
This is also an intuitive choice for painters who are generally familiar with [Munsell Color System](https://en.wikipedia.org/wiki/Munsell_color_system). CIELAB works the same way: lightness on the vertical axis, hue as rotation, and saturation as distance from center.

Next, I needed to figure out how to render this space. Solid meshes only show the surface, and volume raymarching is too expensive for AR (which renders stereo at 1.5x normal framerates). So I turned to particles.

### VFX Particles

I'll tell you one thing: Lens Studio's VFX Editor is fun to use. It's like a stylish combo of Unity's Visual Effects Graph and Blender's GeoNode Editor with some sparks of [Houdini VOP](https://tokeru.com/cgwiki/Houdini_Vops.html) brilliance.

Now, it doesn't come without its own quirks, especially regarding my use case.

The main challenge was encoding the position of every element of the color space so that it could be read by the VFX editor and used to modify particle attributes.

Figuring out a good encoding strategy was tricky. I stumbled on the lack of support for floating point textures in Lens Studio 5.15.
Fortunately, the tutorial [Spawn Particles on Mesh](https://developers.snap.com/lens-studio/features/graphics/mesh-data-texture#spawn-particles-on-mesh) helped with this.
I also had to deal with a peculiarity where the integer index of particles in the VFX Editor is always even. As a result, I had to spawn twice as many particles as those actually displayed.
This was painful to debug, but eventually I settled on a suitable workflow.

The encoder material writes position and color data into render textures. The script orchestrates the pipeline, creating render targets and connecting the material output to the VFX input.
Finally, the VFX decoder spawns particles, sampling the render textures to set each particle's position and color.

<WorkflowDiagram client:load />

Here's the result:

<div style={{ display: "flex", justifyContent: "center", margin: "24px 0" }}>
<video
	controls
	muted
	playsInline
	style={{ maxWidth: "300px", borderRadius: "12px", boxShadow: "0 4px 20px rgba(0,0,0,0.15)" }}
>
	<source src="/assets/visualizing-color-spaces-in-ar-glasses/demo_vfx_particles.mp4" type="video/mp4" />
</video>
</div>

Now, as happy as I was to have overcome those technical hurdles, the performance just wasn't there. Adding 3 such VFX to my scene would visibly reduce framerate.
On Spectacles, this results in a very bad user experience as virtual objects are out of sync with real world elements.
I managed to improve performance a bit by using billboard quads instead of meshes, but it was nowhere near satisfactory.

At this point I hit a wall. I put the project aside and traveled to Brussels and Eindhoven for UnitedXR and the Spectacles x 3EALITY hackathon.

When I came back, <HoverReveal client:load trigger="it hit me" imageSrc="/assets/visualizing-color-spaces-in-ar-glasses/dj-khaled.gif" imageAlt="DJ Khaled - Another one" imageWidth="280px" />: color spaces have interesting **regularities**.
The mapping from RGB to LAB is continuousâ€”nearby colors stay nearby. This is what matters for visualization: if you render grid lines for an RGB cube, you can deform them into LAB space and they'll stretch and bend, but never break.

Here's a [manim](https://github.com/3b1b/manim) visualization that showcases this process:

<div style={{ display: "flex", justifyContent: "center", margin: "16px 0" }}>
<img
	src="/assets/visualizing-color-spaces-in-ar-glasses/color_space_transforms.gif"
	alt="Color space transforms"
	style={{ width: "300px", borderRadius: "8px" }}
/>
</div>

<CollapsibleCode client:load src="/assets/visualizing-color-spaces-in-ar-glasses/animations/color_space_transforms.py" trigger="View manim source" language="python" />

It was clear then that I didn't need a system as flexible as VFX particles to accommodate any arbitrary spatial layout.
I could "simply" define one layout and deform it to achieve the others. This led me to a fourth technique: procedural meshes.

### 4. Procedural Meshes

In Lens Studio, you can [create meshes procedurally](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/3d/procedural-mesh).
Using the [MeshBuilder](https://developers.snap.com/lens-studio/api/lens-scripting/classes/Built-In.MeshBuilder.html) API, you can specify vertex positions and store any attribute you like.
Then, you can assign a material that alters the position of the vertices.
When I tried this, I was mindblown by the performance improvement. 

Though the procedural mesh generation was quite delicate, once done I could essentially let my imagination run wild. 
Moreover, because the vertices are looped over in the GPU via vertex shaders, I didn't have to worry about their count. 

To make the process of low-level geometry construction and manipulation bearable, I used a coding agent supplemented with a <CollapsibleCode client:load src="/assets/visualizing-color-spaces-in-ar-glasses/scripts/custom_code_node_spec.md" trigger="Custom Code Node specification" language="glsl" />. 
Having cleared a path towards seamless rendering of color spaces, I could now tackle the painting workflow pain points mentioned earlier.

### Full Color Space

The complete CIELAB color space rendered as a grid of lines. This answers [question 1](#q-color-range).


<WorkflowDiagramSimple
	client:load
	scriptLabel="Script"
	materialLabel="Material"
	materialCapture="full_color_space.png"
	scriptCodeFile="RGBCubeGenerator.ts"
	materialCodeFile="full_space_mat.txt"
/>

### Pigment Mixing Gamut

The subset of colors reachable by mixing specific pigments. This answers [question 2](#q-gamut).

I tried spawning cubes and repositioning their vertices.
The performance was great, much better than VFX particles rendered as cube meshes. 
I suspect this is due to memory allocation niceties of the MeshBuilder API, coupled with not creating any new geometry at runtime. 

<WorkflowDiagramSimple
	client:load
	scriptLabel="Script"
	materialLabel="Material"
	materialCapture="full_color_space.png"
	scriptCodeFile="PigmentGamutMeshGenerator.ts"
	materialCodeFile="pigment_gamut_mat.txt"
/>

### Pigment Projection

A target color projected onto the gamut boundary. This answers [question 3](#q-projection).

<WorkflowDiagramSimple
	client:load
	scriptLabel="Script"
	materialLabel="Material"
	materialCapture="projector.png"
	scriptCodeFile="GamutProjectionMeshGenerator.ts"
	materialCodeFile="projector_mat.txt"
/>

</Chapter>

## Up next: Image Processing Pipeline with Lens Studio, Snap Cloud x Supabase and Gemini
