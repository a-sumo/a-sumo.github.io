---
title: "Eyedropper for Spectacles AR Glasses"
description: "Building an eyedropper color picker for Snap Spectacles AR glasses using Lens Studio."
pubDatetime: 2025-12-15T07:00:00Z
tags: ["augmented reality", "ar", "spectacles", "lens studio", "ui", "eyedropper"]
ogImage: /assets/eyedropper-for-spectacles-ar-glasses/color_palette_editor_test.gif
icon: /assets/eyedropper-for-spectacles-ar-glasses/eyedropper-logo.png
draft: false
---

import Chapter from "@components/Chapter";
import SeriesNav from "@components/SeriesNav";
import ConditionalLink from "@components/ConditionalLink";
import CollapsibleCode from "@components/CollapsibleCode";
import ArticleNavToggle from "@components/ArticleNavToggle";
import VideoPlayer from "@components/VideoPlayer";

<ArticleNavToggle
  client:load
  defaultView="outline"
  chapters={[
    {
      id: "chapter-1",
      label: "Why an Eyedropper?",
      type: "chapter",
      summary: [
        "Sampling colors from physical palettes",
        "Translating desktop interactions to AR",
      ],
      preview: "/assets/eyedropper-for-spectacles-ar-glasses/sc-eyedropper-chapter-1.png",
    },
    {
      id: "chapter-2",
      label: "Implementation",
      type: "chapter",
      summary: [
        "Reading color data from camera feed",
        "Cropping functionality from Spectacles Samples",
        "Figma-inspired magnified view with grid",
      ],
      preview: "/assets/eyedropper-for-spectacles-ar-glasses/sc-eyedropper-chapter-2.png",
    },
    {
      id: "chapter-3",
      label: "UI and Materials",
      type: "chapter",
      summary: [
        "Spectacles UI Kit for key elements",
        "Custom Code Nodes for shaders",
        "Procedural grid material graph",
      ],
      preview: "/assets/eyedropper-for-spectacles-ar-glasses/sc-eyedropper-chapter-3.png",
    },
    {
      id: "chapter-4",
      label: "Security Considerations",
      type: "chapter",
      summary: [
        "getPixels API restrictions",
        "Remote Service Module authorization",
        "Preventing sensitive data exfiltration",
      ],
      preview: "/assets/eyedropper-for-spectacles-ar-glasses/sc-eyedropper-chapter-4.png",
    },
    {
      id: "chapter-5",
      label: "Comfort and Ease of Use",
      type: "chapter",
      summary: [
        "Mid-air vs tactile interaction",
        "Friction and trajectory stabilization",
        "Multimodal AI as an alternative",
      ],
      preview: "/assets/eyedropper-for-spectacles-ar-glasses/sc-eyedropper-chapter-5.png",
    },
    {
      id: "chapter-6",
      label: "Code Snippets",
      type: "chapter",
      summary: [
        "Procedural Grid shader code",
        "CropAreaSelector methods",
      ],
      preview: "/assets/eyedropper-for-spectacles-ar-glasses/sc-eyedropper-chapter-6.png",
    },
  ]}
/>

<SeriesNav
  client:load
  seriesName="AR Assistant for Painters"
  currentPart={1}
  parts={[
    { title: "Eyedropper for Spectacles", href: "/posts/eyedropper-for-spectacles-ar-glasses" },
    { title: "Visualizing Color Spaces", href: "/posts/visualizing-color-spaces-in-ar-glasses" },
  ]}
/>

<div style={{ textAlign: "center", marginBottom: "8px" }}>
<VideoPlayer
  client:load
  src="/assets/eyedropper-for-spectacles-ar-glasses/color_palette_editor_test.mp4"
  autoPlay={true}
  loop={true}
  muted={true}
  maxWidth="300px"
/>
</div>

<div style={{ display: "flex", justifyContent: "center", alignItems: "center", gap: "24px", flexWrap: "wrap", fontFamily: '"Roboto Mono", monospace', fontSize: "13px" }}>
	<a href="https://github.com/a-sumo/specs-samples/" style={{ display: "inline-flex", alignItems: "center", gap: "8px" }}>
		<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
			<path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/>
		</svg>
		a-sumo/specs-samples
	</a>
	<a href="https://www.spectacles.com/lens/563d15eba7dd42609bdcb6ea641d7940?type=SNAPCODE&metadata=01" style={{ display: "inline-flex", alignItems: "center", gap: "8px" }}>
		<img src="/assets/icons/snapcode-eyedropper.png" width="96" height="96" alt="Snapcode" />
		Try on Spectacles
	</a>
</div>

<Chapter client:visible id="chapter-1" number={1} title="Why an Eyedropper?">

For the past two weeks, I've been working on an augmented reality assistant for traditional painting; a project that fuses my passions for oil painting and XR.
At some point, I needed a way to sample exact colors from my physical palette and bring them into the AR glasses.
On desktop, this is handled by the eyedropper, [a tool](https://en.wikipedia.org/wiki/Color_picker) that lets you read a color at a specific point on screen. I wanted to see how this interaction might translate to AR, so I built one in Lens Studio for the 2024 Spectacles.

</Chapter>

<Chapter client:visible id="chapter-2" number={2} title="Implementation">

The color data is read from the user's camera feed. Because this feed is quite large, and I was planning to sample a single pixel from it, I needed to crop it down. Fortunately, the Spectacles Samples provide a project that implements a cropping functionality that I reused here. 

After the first cropping interaction, an individual pixel might still be too small to be made out by the user. To tackle this issue, I took inspiration from [Figma's eyedropper UI](https://help.figma.com/hc/en-us/articles/27643269375767-Sample-colors-with-the-eyedropper-tool), I implemented a menu containing:
- a magnified view of a sampled area covered with a grid which represents the pixel samples.
- an indicator of the sampled pixel's color.

Both update in real-time, as the user hovers on the crop area's surface.

</Chapter>

<Chapter client:visible id="chapter-3" number={3} title="UI and Materials">

The recently released Spectacles UI Kit took care of many of the key UI elements. This allowed me to focus my attention on bespoke materials for rectangle corners and grids.
My prior 3D development experience involves lots of Three.js, where you'll usually write shaders inline with JavaScript or TypeScript code. As a result, when using Lens Studio I find myself leaning towards writing shader code rather than wrangling material graph nodes. 
For this reason, I've prioritized the use of The Material Graph Editor's Custom Code Nodes.

Below is a screenshot of the Procedural Grid's Material Graph and a snippet of shader code.
<img src="/assets/eyedropper-for-spectacles-ar-glasses/texture_grid_material_editor.png" width="1000" alt="Texture Grid Material Editor" />

The logic for reading and displaying colors is handled by a CropAreaSelector class. I've included some of its core methods below; they showcase ProceduralTextureProvider and [getPixels()](https://developers.snap.com/lens-studio/api/lens-scripting/classes/Built-In.ProceduralTextureProvider.html#getpixels), which are useful for reading and writing pixel data to textures.

</Chapter>

<Chapter client:visible id="chapter-4" number={4} title="Security Considerations">

Note that the getPixels API is [restricted](https://developers.snap.com/lens-studio/features/remote-apis/remote-service-module) when using the Remote Service Module.
As a result, it might prompt a user authorization screen when launching the lens. 

This initially perplexed me: you can send camera textures to a remote server but you can't read pixel values locally? 
The rationale, as best I can understand it is that if you can read pixels locally, you could extract sensitive information (Credit Card data, facial recognition, etc.) and exfiltrate it through innocuous-looking API calls without the user's knowledge. By restricting local pixel access when remote services are enabled, Snap ensures that if any image analysis happens, either:
- it stays entirely on-device (no remote module), or
- the user is explicitly warned that data is leaving the device (remote module triggers authorization prompt)

</Chapter>

<Chapter client:visible id="chapter-5" number={5} title="Comfort and Ease of Use">

When testing the Eyedropper on Spectacles, I noticed that mid-air and fine-grained movements don't really go hand in hand.
Something I've taken for granted from desktop use is that tactile interaction, where the user applies pressure on a surface, results in a tangent friction force. This friction force stabilizes their hand trajectories, both in position and speed.

<img src="/assets/eyedropper-for-spectacles-ar-glasses/tactile_interaction_forces.png" width="300" alt="Tactile Interaction Forces" />

It is precisely this trajectory stabilization which enables finer-grained movement. 
This is not something that can be reproduced through [filtering](https://github.com/casiez/OneEuroFilter), because filtering causes spatial information loss. Friction on the other hand guides movement towards more stable dynamics without causing information loss.

Upon observing this, I felt tempted to drop physical interaction in favor of the most convenient user experience of all: querying a multimodal AI model.

I called a Gemini model to segment palette color blobs and extract pigments from each. Then I actually didn't bother segmenting and simply sent the entire image to a Gemini model and the results were equivalent. 

This would've been unthinkable 5 years ago, but because tech evolves at an exponential pace, 5 years is actually... kind of an eternity.
<img src="/assets/eyedropper-for-spectacles-ar-glasses/plein-air-Celeste-Bergin.jpg" width="300" alt="Gemini Call Input" />
<img src="/assets/eyedropper-for-spectacles-ar-glasses/gemini_call_result.png" width="300" alt="Gemini Call Result" />


Now, though the extracted pigments aren't necessarily found in the input image, it provides enough perceptual accuracy for the user to accomplish their goal. After all, the domain we're tackling is traditional painting and not professional color grading. 

In the end, I will make both interaction modes available for the user to choose: 
- one that is deterministic and over which they have complete control.
- another that is probabilistic and where the user yields a lot of control.

</Chapter>

<Chapter client:visible id="chapter-6" number={6} title="Code Snippets and References">

<CollapsibleCode
  client:visible
  src="/assets/eyedropper-for-spectacles-ar-glasses/scripts/procedural_grid.glsl"
  trigger="Procedural Grid Custom Code Node"
  language="glsl"
/>

<CollapsibleCode
  client:visible
  src="/assets/eyedropper-for-spectacles-ar-glasses/scripts/crop_area_selector.ts"
  trigger="CropAreaSelector Methods"
  language="typescript"
/>

</Chapter>

## Up next

<ConditionalLink
  client:visible
  href="/posts/visualizing-color-spaces-in-ar-glasses"
>
  Visualizing Color Spaces in Augmented Reality with Spectacles
</ConditionalLink>