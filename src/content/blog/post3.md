---
title: "Title"
description: ""
pubDatetime: 2025-12-15T09:00:00Z
tags: ["augmented reality"]
draft: true
---

Striking a balance between embodied interaction and AI model query

Every time the user performs a manual action, they execute "intentional" micro-actions that get stored in procedural memory. 
This engages multiple brain areas and ends up reinforcing learning. 
However, when querying AI models, users invest most effort in formulating intent through natural language, but the embodied component is largely absent.

If we could somehow either make AI model's procedural primitives intelligible to the user and accessible as they are streamed, or if we could encode functional primitives in continuous spatial actions, perhaps we'd get closer to something that combines the advantages of the manual and the AI qiuery approaches.

Interestingly, this is what [neural wristbands](https://www.meta.com/en-gb/emerging-tech/emg-wearable-technology/) are approaching. They intercept continuous electrical signals sent by the brain to the hands hand movements, from which typing or hand-writing can be inferred. But both typing and hand writing operate on letters. The question is whether one can intercept such signals can be used to operate on functions. This would allow me to specify precisely a complex intent with very compact gestures while avoiding the probabilistic natre of AI's interpretaton of intent. 

I'd love to hear thoughts on this subject.
