---
title: "Visualizing Color Spaces in Augmented Reality with Spectacles"
description: "Building interactive 3D color space visualizations for Snap Spectacles AR glasses to help painters understand pigment mixing and color gamuts."
pubDatetime: 2025-12-22T09:00:00Z
tags:
  [
    "augmented reality",
    "ar",
    "spectacles",
    "lens studio",
    "ui",
    "color space",
    "vfx editor",
  ]
ogImage: /assets/visualizing-color-spaces-in-ar-glasses/full_demo_og.gif
icon: /assets/visualizing-color-spaces-in-ar-glasses/ghost_color_space.png
draft: false
---

import WorkflowDiagram from "@components/WorkflowDiagram";
import WorkflowDiagramSimple from "@components/WorkflowDiagramSimple";
import ColorMixingChallenge from "@components/ColorMixingChallenge";
import CollapsibleVideo from "@components/CollapsibleVideo";
import HoverReveal from "@components/HoverReveal";
import CollapsibleCode from "@components/CollapsibleCode";
import Chapter from "@components/Chapter";

<div style="text-align: center;">
	<a
		href="https://github.com/a-sumo/specs-samples/"
		style="display: inline-flex; align-items: center; gap: 10px;"
	>
		<img
			src="/assets/visualizing-color-spaces-in-ar-glasses/ghost_color_space.png"
			width="80"
			alt="Color Space Sample Project"
		/>
		<span>a-sumo/specs-samples</span>
	</a>
</div>

<div style={{ display: "flex", justifyContent: "center", margin: "32px 0" }}>
<video
	autoPlay
	loop
	muted
	playsInline
	style={{ maxWidth: "300px", borderRadius: "12px", boxShadow: "0 4px 20px rgba(0,0,0,0.15)" }}
>
	<source src="/assets/visualizing-color-spaces-in-ar-glasses/full_demo.mp4" type="video/mp4" />
</video>
</div>

<Chapter client:load number={1} title="The Challenge of Color Mixing">

One of the first steps a painter takes is that of assembling together the colors that will be laid on the canvas.
This involves selecting a set of pigments, laying them out on a palette and mixing them. 
During this process, each decision moves the color of the paint across three dimensions: hue, saturation(chromaicity) and value (lightness).
In theory, these dimensions are independent, or orthogonal. This means you can change the value along one dimension while keeping others constant.
Most digital color pickers are built on this assumption: you can pick a hue on one axis and adjust saturation and lightness on another.

<div style={{ textAlign: "center", margin: "16px 0" }}>
<img
	src="/assets/visualizing-color-spaces-in-ar-glasses/figma_hsl_color_picker.png"
	alt="Figma HSL color picker"
	style={{ width: "200px", borderRadius: "8px" }}
/>
<p style={{ fontSize: "14px", color: "rgb(100, 100, 100)", marginTop: "8px" }}>Figma's HSL color picker: hue on one slider, saturation and lightness on a 2D plane.</p>
</div>

Physical pigments, however are not as simple. For example, in order to de-saturate the green color of a tree as it vanishes into the horizon, you'll generally want to add a colder color to it, such as blue.
But by adding blue, you also shift the hue towards... blue! This means that in practice, the dimensions aren't actually independent.
It turns out that mixing color pigments involves a kind of multidimensional dance, which experienced painters perform seamlessly.
Performing this process adequately has an impact on the confidence of your brushstrokes and the overall fluidity of your artwork.
In fact, the more problems the painter can solve ahead of laying their brush on the canvas, the more enjoyable the process for them and the more free-flowing your painting will _feel_ to the viewer.

Now, some shortcuts can be taken, namely using very few or lots of pigments.
If you have too few pigments, you have little expressiveness and it may be unclear whether a certain color in your reference is even achievable.
On the other hand, the more pigments you have, the more degrees of freedom, and hence more complexity. 
This complexity grows combinatorially: not only do you have more possible mixtures to consider, but many different mixtures can yield the same color!
This makes it much harder to reason about which path to take. For a beginner, it's recipe for disaster.

Wanna try? Here: I'll give you a color and you need to find out which mix achieves it!

<CollapsibleVideo client:load src="/assets/visualizing-color-spaces-in-ar-glasses/videos/bonne_chance_franco-succession.mp4" preText='"' trigger="Bonne chance!" postText='"' description="Succession, Season 3 Episode 5" />

<ColorMixingChallenge client:load targetColor="#8B4513" />

<CollapsibleVideo
	client:load
	src="/assets/visualizing-color-spaces-in-ar-glasses/videos/excellent_wonderful_news.mp4"
	preText='How was it? "'
	trigger="Wonderful"
	postText='"?'
	description="Succession, Season 3 Episode 5"
/>

It should be clear, now, why  mastering the pigment mixing process takes years of practice.
You just can't develop an intuition for how specific color pigments will interact with one another across hue saturation and value just by watching video tutorials and reading books. You must paint, again and again, ideally with some guidance, but knowing that the process will be tedious.

But how tedious does it really _need_ to be? Is frustration necessary for learning, or can skillful effort be sufficient?

This brings us to our problem statement:

<div id="problem-statement" style="background: rgb(255, 248, 222); border: 2px solid rgb(40, 39, 40); padding: 20px 24px; borderRadius: 8px; margin: 24px 0;">
<ol style="margin: 0; paddingLeft: 20px; color: rgb(40, 39, 40); lineHeight: 1.8;">
<li id="q-color-range"><strong>What does the full range of perceivable colors look like?</strong></li>
<li id="q-gamut"><strong>Which colors can I achieve with the pigments on my palette?</strong></li>
<li id="q-projection"><strong>How close can I get to a target color given my available pigments?</strong></li>
</ol>
</div>

</Chapter>

<Chapter client:load number={2} title="Building Color Spaces">

The kind of problem I described above is well-suited for [Seeing Spaces](https://worrydream.com/SeeingSpaces/): an environment that reifies and renders interactive abstract concepts, letting you see the full range of possibilities at once rather than guessing blindly.

In our case, a seeing space or rather seeing tool would be one that let's us see what happens across each color dimension as we mix the paint. In other words, a color space.
There are a wide variety of color spaces, but the ones that matter for painting are those that are perceptually accurate.
That's because transitions between colors end up informing mixing decisions.

But seeing spaces only _really_ work when they're seamlessly integrated into the user's environment. They must also be highly responsive and conceptually powerful.
That's a lot to ask from a smartphone, tablet or desktop. None of these devices can be integrated into the painting flow without disrupting it.

Fortunately, we are in the age of AR Glasses I got a pair of [Spectacles AR glasses](https://www.spectacles.com/).

The next thing I needed to figure out was which content to display and how to display it.
Concretely, I wanted some way of knowing where a specific color on my palette sits in the color space, so that when I change its mixture, I can see the result.
I also wanted to know which colors are accessible based on the paint drops available. The name for that is a [color gamut](https://en.wikipedia.org/wiki/Gamut).
Finally, I wanted to see how a specific target color maps into this gamut: what's the closest I can get to a reference color based on the pigments at my disposal?

RGB is the obvious starting point, but it's a poor fit for this problem. It's perceptually inaccurate. [CIELAB](https://en.wikipedia.org/wiki/CIELAB_color_space), for instance, is better suited: it's designed so that equal distances in the space correspond to equal perceived color differences. 
This is also an intuitive choice for painters who are generally familiar with [Munsell Color System](https://en.wikipedia.org/wiki/Munsell_color_system). CIELAB works the same way: lightness on the vertical axis, hue as rotation, and saturation as distance from center.

Next, I needed to figure out how to represent this space. I considered 4 main approaches:

### 1. Solids

Solids are very useful when you want to see how a convex set such the full color gamut of a display.
Because the surface of the domain matters as much as the interior, this approach is not particularly useful.

### 2. Volumes

These are generally rendered via volume raymarching. Maxime Heckel has written [a gorgeous article on the subject](https://blog.maximeheckel.com/posts/real-time-cloudscapes-with-volumetric-raymarching/) if you're interested.

It's ideal to single out a subdomain of 3D volumetric data such as bones, which are more dense than surrounding flesh.
But in the case of color spaces, each element has equal importance. It would be improper to integrate samples of density values over a ray path into a screen-projected value, as is done in raycasting.

Additionally, the raycasting loop is expensive to compute. It only runs smoothly (no performance throttling) on 2024 Spectacles if you drop the volume grid's resolution. 

Note that because AR glasses need to render one image for each eye at 1.5x the framerate of non head-worn devices for viewing comfort, performance is a real constraint.

### 3. VFX Particles

I'll tell you one thing: Lens Studio's VFX Editor is fun to use. It's like a stylish combo of Unity's VFX Graph and Blender's GeoNode Editor.

Now, it doesn't come without its own quirks, especially regarding my use case.

The main challenge was encoding the position of every element of the color space so that it could be read by the VFX editor and used to modify particle attributes.

Figuring out a good encoding strategy was tricky. I stumbled on the lack of support for floating point textures in Lens Studio 5.15.
Fortunately, the tutorial [Spawn Particles on Mesh](https://developers.snap.com/lens-studio/features/graphics/mesh-data-texture#spawn-particles-on-mesh) helped with this.
I also had to deal with a quirk where the integer index of particles in the VFX Editor is always even. As a result, I had to spawn twice as many particles as those actually displayed.
This was painful to debug, but eventually I settled on a suitable workflow.

The encoder material writes position and color data into render textures. The script orchestrates the pipeline, creating render targets and connecting the material output to the VFX input.
Finally, the VFX decoder spawns particles, sampling the render textures to set each particle's position and color.

<WorkflowDiagram client:load />

Now, as happy as I was of having overcome those technical hurdles, the performance just wasn't there. Adding 3 such VFX to my scene would visibly reduce framerate.
On Spectacles, this results in a very bad user experience as virtual objects are out of sync with real world elements.
I managed to improve performance a bit by using billboard quads instead of meshes, but it was nowhere near satisfactory.

It's at this point that I began to question the whole 3D color space visualization approach.
Was there something I was missing?

As I stepped back to think, I noticed that color spaces have interesting **regularities**.
Each element in Color Space A has a unique mapped element in Color Space B, a property known as bijectivity.
This means that if you render out, say, grid lines for an RGB cube, you can simply deform them to get the resulting lines in the destination space.
Here's a [manim](https://github.com/3b1b/manim) visualization that showcases this process:
<img
	src="/assets/visualizing-color-spaces-in-ar-glasses/color_space_transforms.gif"
	alt="Color space transforms"
	style={{ width: "300px", borderRadius: "8px" }}
/>
It was clear then that I didn't need a system as flexible as VFX particles to accommodate any arbitrary spatial layout.
I could figure out one layout then deform it to achieve the others. This property of bijectivity led me to a fourth technique.

### 4. Procedural Meshes

In Lens Studio, you can [create meshes procedurally](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/3d/procedural-mesh).
Using the [MeshBuilder](https://developers.snap.com/lens-studio/api/lens-scripting/classes/Built-In.MeshBuilder.html) API, you can specify vertex positions and store any attribute you like.
Then, you can assign a material that alters the position of the vertices.
When I tried this, I was mindblown by the performance improvement. 

Though the procedural mesh generation was quite delicate, once done I could essentially let my imagination run wild. 
Moreover, because the vertices are looped over in the GPU via vertex shaders, I didn't have to worry about their count. 

To make the process of low-level geometry construction and manipulation bearable, I used a coding agent supplemented with a <CollapsibleCode client:load src="/assets/visualizing-color-spaces-in-ar-glasses/scripts/custom_code_node_spec.md" trigger="Custom Code Node specification" language="glsl" />. 
Having cleared a path towards seamless rendering of color spaces, I could now tackle the painting workflow pain points mentioned earlier.

### Full Color Space

The complete CIELAB color space rendered as a grid of lines. This answers [question 1](#q-color-range).


<WorkflowDiagramSimple
	client:load
	scriptLabel="Script"
	materialLabel="Material"
	materialCapture="full_color_space.png"
	scriptCodeFile="RGBCubeGenerator.ts"
	materialCodeFile="full_space_mat.txt"
/>

### Pigment Mixing Gamut

The subset of colors reachable by mixing specific pigments. This answers [question 2](#q-gamut).

I tried spawning cubes and repositioning their vertices.
The performance was great, much better than VFX particles rendered as cube meshes. 
I suspect this is due to memory allocation niceties of the MeshBuilder API, coupled with not creating any new geometry at runtime. 

<WorkflowDiagramSimple
	client:load
	scriptLabel="Script"
	materialLabel="Material"
	materialCapture="full_color_space.png"
	scriptCodeFile="PigmentGamutMeshGenerator.ts"
	materialCodeFile="pigment_gamut_mat.txt"
/>

### Pigment Projection

A target color projected onto the gamut boundary. This answers [question 3](#q-projection).

<WorkflowDiagramSimple
	client:load
	scriptLabel="Script"
	materialLabel="Material"
	materialCapture="projector.png"
	scriptCodeFile="GamutProjectionMeshGenerator.ts"
	materialCodeFile="projector_mat.txt"
/>

</Chapter>

## Up next: Image Processing Pipeline with Lens Studio, Snap Cloud x Supabase and Gemini
